# vim: ft=yaml
# yamllint disable rule:comments-indentation
# yamllint disable rule:line-length
---
archivebox:
  install:
      # Install rootless containers running as dedicated user.
    rootless: true
      # Add autoupdate label to containers, allowing to run
      # podman autoupdate (manually)
    autoupdate: true
      # Manage podman-auto-update.timer (auto-autoupdate)
      # (null=do not manage, false=disable, true=enable)
    autoupdate_service: false
      # When applying `clean` states, also remove application data.
      # Setting this to true might incur accidental data loss!
    remove_all_data_for_sure: false
  config:
    # general:
          # Toggle whether or not to attempt rechecking old links when adding new ones,
          # or leave old incomplete links alone and only archive the new links.
          # By default, ArchiveBox will only archive new links on each import. If you
          # want it to go back through all links in the index and download any missing
          # files on every run, set this to False. Regardless of how this is set,
          # ArchiveBox will never re-download sites that have already succeeded
          # previously. When this is False it only attempts to fix previous pages have
          # missing archive extractor outputs, it does not re-archive pages that have
          # already been successfully archived.
    #   only_new: true
          # Maximum allowed download time per archive method for each link in seconds.
    #   timeout: 60
          # Maximum allowed download time for fetching media when SAVE_MEDIA=True [in s]
    #   media_timeout: 3600
          # Permissions to set the output directory and file contents to.
    #   output_permissions: '644'
          # A regex expression used to exclude certain URLs from archiving.
          # All assets required to render each page are still archived.
    #   url_blacklist: \.(css|js|otf|ttf|woff|woff2|gstatic\.com|googleapis\.com/css)(\?.*)?$
          # A regex expression used to exclude all URLs that don't match the given
          # pattern from archiving. This option is useful for recursive archiving of all
          # the pages under a given domain or subfolder (crawling/spidering).
    #   url_whitelist: null
          # Allows disabling forced FSYNC writes (on network drives). If enabled
          # (default), the main data directory needs to reside on a local drive.
    #   enforce_atomic_writes: true
          # REGEX that specifies how to split a string of tags into a list.
    #   tag_separator_pattern: '[,]'
          # This is the root data folder where Archivebox.conf, index.sqlite3 and
          # archive/* logs/* sources/* reside. You should not set this.
    #   output_dir: null
          # Path to the configuration file. You should not set this unless you know
          # what you are doing.
    #   config_file: null

    search_backend:
      search_backend_host_name: sonic
      search_backend_password: null
      # use_indexing_backend: true
      # use_searching_backend: true
      # search_backend_engine: ripgrep
      # search_backend_port: 1491
      # sonic_collection: archivebox
      # sonic_bucket: snapshots
      # search_backend_timeout: 90

    server:
      bind_addr: 0.0.0.0:4180
      secret_key: null
      # allowed_hosts: '*'
      # debug: false
          # Allow anon users to view the index.
      # public_index: true
          # Allow anon users to view snapshots.
      # public_snapshots: true
          # Allow anon users to add snapshots.
      # public_add_view: false
          # Some text to display in the footer of the archive index.
      # footer_info: >-
      #   Content is hosted for personal archiving purposes only.  Contact server
      #   owner for any takedown requests.
          # Maximum number of Snapshots to show per page on Snapshot list pages.
      # snapshots_per_page: 40
          # Path to a directory containing custom html/css/images for overriding
          # the default UI styling
      # custom_templates_dir: null
      # timezone: UTC
          # Toggle iframe previews of the original URLs.
          # Snapshot index pages are static html, so you'll have to trigger them
          # to be rerendered manually to update them, either by clicking Pull Title,
          # Pull, or running archivebox update on them.
      # preview_originals: true

    # archive_methods:
          # By default ArchiveBox uses the title provided by the import file, but
          # not all types of imports provide titles (e.g. Plain texts lists of URLs).
          # When this is True, ArchiveBox downloads the page (and follows all
          # redirects), then it attempts to parse the link's title from the first
          # <title></title> tag found in the response.
    #   save_title: true
          # Fetch and save favicon for the URL from Google's public favicon service.
    #   save_favicon: true
          # Fetch page with wget, and save responses into folders for each domain.
    #   save_wget: true
          # Fetch an HTML file with all assets embedded using Single File.
          # (https://github.com/gildas-lormeau/SingleFile)
    #   save_singlefile: true
          # Extract article text, summary, and byline using Mozilla's Readability
          # library. Unlike the other methods, this does not download any additional
          # files, so it's practically free from a disk usage perspective.
    #   save_readability: true
          # Extract article text, summary, and byline using the Mercury library.
          # Unlike the other methods, this does not download any additional files,
          # so it's practically free from a disk usage perspective.
    #   save_mercury: true
          # Print page as PDF.
    #   save_pdf: true
          # Fetch a screenshot of the page.
    #   save_screenshot: true
          # Fetch a DOM dump of the page.
    #   save_dom: true
    #   save_headers: true
          # Save a timestamped WARC archive of all the page requests and responses
          # during the wget archive process.
    #   save_warc: true
          # Fetch any git repositories on the page.
    #   save_git: true
          # Fetch all audio, video, annotations, and media metadata on the page
          # using youtube-dl.
    #   save_media: true
          # Submit the page's URL to be archived on Archive.org.
    #   save_archive_dot_org: true

    # archive_method_options:
          # Screenshot resolution in pixels width,height.
    #   resolution: '1440,2000'
          # Domains to attempt download of git repositories on using git clone.
    #   git_domains: 'github.com,bitbucket.org,gitlab.com,gist.github.com'
          # Whether to enforce HTTPS certificate and HSTS chain of trust when archiving
          # sites. Set this to False if you want to archive pages even if they have
          # expired or invalid certificates.  Be aware that when False you cannot
          # guarantee that you have not been man-in-the-middle'd while archiving
          # content, so it cannot be verified to be what's on the original site.
    #   check_ssl_validity: true
          # Fetch images/css/js with wget. (This should be true in most cases.)
          # [dev note: This is actually found in archive_methods,
          # but sections don't matter to ArchiveBox.]
    #   save_wget_requisites: true
          # This is a wget CLI option:
          # Change which characters found in remote URLs must be escaped during
          # generation of local filenames.  Characters that are restricted by
          # this option are escaped, i.e. replaced with %HH, where HH is the
          # hexadecimal number that corresponds to the restricted character.
          # [dev note: This is actually found in general]
    #   restrict_file_names: windows
    #   media_max_size: 750m
          # This is the user agent to use during curl/wget archiving. You can set
          # this to impersonate a more common browser like Chrome or Firefox if
          # you're getting blocked by servers for having an unknown/blacklisted
          # user agent.
    #   curl_user_agent: >-
    #     Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML,
    #     like Gecko) Chrome/102.0.0.0 Safari/605.1.15 ArchiveBox/{VERSION}
    #     (+https://github.com/ArchiveBox/ArchiveBox/) curl/{CURL_VERSION}
    #   wget_user_agent: >-
    #     Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML,
    #     like Gecko) Chrome/102.0.0.0 Safari/605.1.15 ArchiveBox/{VERSION}
    #     (+https://github.com/ArchiveBox/ArchiveBox/) wget/{WGET_VERSION}
          #  If you're experiencing being blocked by many sites, you can set this to
          # hide the Headless string that reveals to servers that you're using a
          # headless browser.
    #   chrome_user_agent: >-
    #     Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML,
    #     like Gecko) Chrome/102.0.0.0 Safari/605.1.15 ArchiveBox/{VERSION}
    #     (+https://github.com/ArchiveBox/ArchiveBox/)
          # Cookies file to pass to wget. To capture sites that require a user to be
          # logged in, you can specify a path to a netscape-format cookies.txt file
          # for wget to use. You can generate this file by using a browser extension to
          # export your cookies in this format, or by using wget with --save-cookies.
    #   cookies_file: null
          # Path to a Chrome user profile directory. To capture sites that require a
          # user to be logged in, you can specify a path to a chrome user profile
          # (which loads the cookies needed for the user to be logged in). If you don't
          # have an existing Chrome profile, create one with
          # chromium-browser --user-data-dir=/tmp/chrome-profile, and log into the sites
          # you need. Then set CHROME_USER_DATA_DIR=/tmp/chrome-profile to make
          # ArchiveBox use that profile.
    #   chrome_user_data_dir: null
          # Whether or not to use Chrome/Chromium in --headless mode (no browser UI
          # displayed). When set to False, the full Chrome UI will be launched each time
          # it's used to archive a page, which greatly slows down the process but allows
          # you to watch in real-time as it saves each page.
    #   chrome_headless: true
          # This is a security-critical setting, only set this to False if you're
          # running ArchiveBox inside a container or VM where it doesn't have access to
          # the rest of your system.
    #   chrome_sandbox: false
    #   youtubedl_args:
    #     - '--write-description'
    #     - '--write-info-json'
    #     - '--write-annotations'
    #     - '--write-thumbnail'
    #     - '--no-call-home'
    #     - '--write-sub'
    #     - '--all-subs'
    #     - '--write-auto-sub'
    #     - '--convert-subs=srt'
    #     - '--yes-playlist'
    #     - '--continue'
    #     - '--ignore-errors'
    #     - '--no-abort-on-error'
    #     - '--geo-bypass'
    #     - '--add-metadata'
    #     - '--max-filesize=750m'
    #   wget_args:
    #     - '--no-verbose'
    #     - '--adjust-extension'
    #     - '--convert-links'
    #     - '--force-directories'
    #     - '--backup-converted'
    #     - '--span-hosts'
    #     - '--no-parent'
    #     - '-e'
    #     - robots=off
    #   curl_args:
    #     - '--silent'
    #     - '--location'
    #     - '--compressed'
    #   git_args:
    #     - '--recursive'

    # dependencies:
    #   use_curl: true
    #   use_wget: true
    #   use_singlefile: true
    #   use_readability: true
    #   use_mercury: true
    #   use_git: true
    #   use_chrome: true
    #   use_node: true
    #   use_youtubedl: true
    #   use_ripgrep: true
    #   curl_binary: curl
    #   git_binary: git
    #   wget_binary: wget
    #   singlefile_binary: /node/node_modules/.bin/single-file
    #   readability_binary: /node/node_modules/.bin/readability-extractor
    #   mercury_binary: /node/node_modules/.bin/mercury-parser
    #   youtubedl_binary: yt-dlp
    #   node_binary: node
          # Optionally switch this to use ripgrep-all for full-text search support
          # across more filetypes (e.g. PDF).
    #   ripgrep_binary: rg
    #   chrome_binary: /usr/bin/chromium-browser
    #   pocket_consumer_key: null
    #   pocket_access_tokens: {}

  pihole:
    config:
        # Password for the Pihole Web UI. Will be autogenerated
        # if undefined.
      webpassword: null
    enable: false
      # Port on host to map to Pihole Web UI.
    webui_port: 1073
  pywb:
    enable: false

  sonic:
      # Enable sonic as search backend. If you enable it later, you will need to
      # backfill existing Snapshots into the index:
      # docker-compose run archivebox update --index-only
    enable: true
      # This configuration is taken from the official example at
      # https://github.com/ArchiveBox/ArchiveBox/blob/03eb7e58758d8dcb85ed781e713fc083f8292264/etc/sonic.cfg
      # You should not need to modify it.
    config:
      channel:
        auth_password: ${env.SEARCH_BACKEND_PASSWORD}
        inet: 0.0.0.0:1491
        search:
          query_alternates_try: 10
          query_limit_default: 65535
          query_limit_maximum: 65535
          suggest_limit_default: 5
          suggest_limit_maximum: 20
        tcp_timeout: 300
      server:
        log_level: warn
      store:
        fst:
          graph:
            consolidate_after: 180
            max_size: 2048
            max_words: 250000
          path: /var/lib/sonic/store/fst/
          pool:
            inactive_after: 300
        kv:
          database:
            compress: true
            flush_after: 900
            max_compactions: 1
            max_files: 100
            max_flushes: 1
            parallelism: 2
            write_ahead_log: true
            write_buffer: 16384
          path: /var/lib/sonic/store/kv/
          pool:
            inactive_after: 1800
          retain_word_objects: 100000

    # When using Pihole to block ads, wget extractor gets stuck trying to resolve
    # IPv6 addresses for blocked hosts. This can be fixed in several ways,
    # this one just disables ipv6 for the Archivebox container completely.
    # A less intrusive way is to add --ipv4-only to WGET_ARGS.
    # see https://github.com/ArchiveBox/ArchiveBox/issues/865
  wget_dns_fix_disable_ipv6: false

  lookup:
    rootgroup: root
    compose:
      create_pod: null
      pod_args: null
      project_name: archivebox
      remove_orphans: true
      build: false
      build_args: null
      pull: false
      service:
        container_prefix: null
        ephemeral: true
        pod_prefix: null
        restart_policy: on-failure
        restart_sec: 2
        separator: null
        stop_timeout: null
    paths:
      base: /opt/containers/archivebox
      compose: docker-compose.yml
      config_archivebox: archivebox.env
      config_sonic: sonic.env
      config_pihole: pihole.env
      config_pywb: pywb.env
      data: data
      data_archive: archive
      data_dnsmasq: dnsmasq
      data_pihole: pihole
      data_pywb: pywb
      data_sonic: sonic
      sonic_cfg: sonic.cfg
    user:
      groups: []
      home: null
      name: archivebox
      shell: /usr/sbin/nologin
      uid: null
      gid: null
    containers:
      archivebox:
        image: docker.io/archivebox/archivebox:latest
      pihole:
        image: docker.io/pihole/pihole:latest
      pywb:
        image: docker.io/webrecorder/pywb:latest
      sonic:
        image: docker.io/valeriansaliou/sonic:v1.3.1

  tofs:
      # The files_switch key serves as a selector for alternative
      # directories under the formula files directory. See TOFS pattern
      # doc for more info.
      # Note: Any value not evaluated by `config.get` will be used literally.
      # This can be used to set custom paths, as many levels deep as required.
    files_switch:
      - any/path/can/be/used/here
      - id
      - roles
      - osfinger
      - os
      - os_family

      # All aspects of path/file resolution are customisable using the options below.
      # This is unnecessary in most cases; there are sensible defaults.
      # Default path: salt://< path_prefix >/< dirs.files >/< dirs.default >
      # I.e.: salt://archivebox/files/default
      # path_prefix: template_alt
      # dirs:
      #   files: files_alt
      #   default: default_alt
      # The entries under `source_files` are prepended to the default source files
      # given for the state
    source_files:
      ArchiveBox compose file is managed:
        - docker-compose_alt.yml
        - docker-compose_alt.yml.j2
      archivebox environment file is managed:
        - archivebox_alt.env
        - archivebox_alt.env.j2
      sonic environment file is managed:
        - sonic_alt.env
        - sonic_alt.env.j2
      pihole environment file is managed:
        - pihole_alt.env
        - pihole_alt.env.j2
      pywb environment file is managed:
        - pywb_alt.env
        - pywb_alt.env.j2
      Sonic config file is managed:
        - sonic_alt.cfg
        - sonic_alt.cfg.j2
